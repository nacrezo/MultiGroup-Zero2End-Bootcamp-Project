{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Optimization\n",
        "\n",
        "## Amaç\n",
        "\n",
        "Seçtiğiniz clustering modellerine hiperparametre optimizasyonu uygulayın ve en iyi parametreleri bulun.\n",
        "\n",
        "### Optimizasyon Stratejisi\n",
        "- **K-Means**: n_clusters, init, n_init, max_iter\n",
        "- **DBSCAN**: eps, min_samples\n",
        "- **Hierarchical**: n_clusters, linkage\n",
        "- **Grid Search / Random Search** ile optimal parametreleri bulma\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "import warnings\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.append(str(Path('..').resolve()))\n",
        "from src.config import *\n",
        "from src.data_loader import load_gaming_dataset, create_sample_gaming_dataset\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"Libraries imported!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Veri Yükleme ve Hazırlık\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "try:\n",
        "    df = load_gaming_dataset(RAW_DATA_DIR)\n",
        "    if df is None or len(df) == 0:\n",
        "        raise FileNotFoundError(\"Dataset not found\")\n",
        "except FileNotFoundError:\n",
        "    df = create_sample_gaming_dataset(n_samples=20000, save_path=TRAIN_FILE)\n",
        "\n",
        "# Feature engineering (baseline'dan sonraki feature set)\n",
        "from src.pipeline import UserSegmentationPipeline\n",
        "pipeline_temp = UserSegmentationPipeline()\n",
        "df_processed = pipeline_temp.preprocess(df, is_training=True)\n",
        "\n",
        "# Select numerical features\n",
        "numerical_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "X = df_processed[numerical_cols].fillna(df_processed[numerical_cols].median())\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(f\"Data prepared: {X_scaled.shape}\")\n",
        "print(f\"Features: {len(numerical_cols)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K-Means Hiperparametre Optimizasyonu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-Means parametre optimizasyonu\n",
        "param_grid = {\n",
        "    'n_clusters': [3, 4, 5, 6, 7],\n",
        "    'init': ['k-means++', 'random'],\n",
        "    'n_init': [10, 20, 30],\n",
        "    'max_iter': [300, 500]\n",
        "}\n",
        "\n",
        "results = []\n",
        "print(\"K-Means optimizasyonu yapılıyor...\")\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "    kmeans = KMeans(random_state=MODEL_CONFIG['random_state'], **params)\n",
        "    labels = kmeans.fit_predict(X_scaled)\n",
        "    \n",
        "    sil_score = silhouette_score(X_scaled, labels)\n",
        "    db_score = davies_bouldin_score(X_scaled, labels)\n",
        "    ch_score = calinski_harabasz_score(X_scaled, labels)\n",
        "    \n",
        "    results.append({\n",
        "        **params,\n",
        "        'silhouette_score': sil_score,\n",
        "        'davies_bouldin': db_score,\n",
        "        'calinski_harabasz': ch_score,\n",
        "        'inertia': kmeans.inertia_\n",
        "    })\n",
        "    print(f\"n_clusters={params['n_clusters']}, init={params['init']}, n_init={params['n_init']}: Silhouette={sil_score:.4f}\")\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nEn iyi parametreler:\")\n",
        "print(results_df.nlargest(5, 'silhouette_score')[['n_clusters', 'init', 'n_init', 'max_iter', 'silhouette_score', 'davies_bouldin']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DBSCAN Optimizasyonu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DBSCAN parametre optimizasyonu\n",
        "dbscan_params = {\n",
        "    'eps': [0.3, 0.5, 0.7, 1.0, 1.5],\n",
        "    'min_samples': [3, 5, 7, 10]\n",
        "}\n",
        "\n",
        "dbscan_results = []\n",
        "print(\"DBSCAN optimizasyonu yapılıyor...\")\n",
        "\n",
        "for eps in dbscan_params['eps']:\n",
        "    for min_samples in dbscan_params['min_samples']:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(X_scaled)\n",
        "        \n",
        "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "        n_noise = list(labels).count(-1)\n",
        "        \n",
        "        if n_clusters > 1:  # En az 2 cluster olmalı\n",
        "            sil_score = silhouette_score(X_scaled, labels)\n",
        "            db_score = davies_bouldin_score(X_scaled, labels)\n",
        "            \n",
        "            dbscan_results.append({\n",
        "                'eps': eps,\n",
        "                'min_samples': min_samples,\n",
        "                'n_clusters': n_clusters,\n",
        "                'n_noise': n_noise,\n",
        "                'silhouette_score': sil_score,\n",
        "                'davies_bouldin': db_score\n",
        "            })\n",
        "            print(f\"eps={eps}, min_samples={min_samples}: n_clusters={n_clusters}, Silhouette={sil_score:.4f}\")\n",
        "\n",
        "if dbscan_results:\n",
        "    dbscan_df = pd.DataFrame(dbscan_results)\n",
        "    print(\"\\nEn iyi DBSCAN parametreleri:\")\n",
        "    print(dbscan_df.nlargest(5, 'silhouette_score'))\n",
        "else:\n",
        "    print(\"DBSCAN ile uygun parametreler bulunamadı.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Optimization Docs\n",
        "\n",
        "### Optimize Edilen Modeller\n",
        "\n",
        "1. **K-Means**: Grid search ile n_clusters, init, n_init, max_iter optimize edildi\n",
        "2. **DBSCAN**: eps ve min_samples parametreleri test edildi\n",
        "\n",
        "### Sonuçlar\n",
        "\n",
        "- **En iyi K-Means parametreleri**: [Sonuçlara göre doldurulacak]\n",
        "- **En iyi DBSCAN parametreleri**: [Sonuçlara göre doldurulacak]\n",
        "- **Seçilen model**: [K-Means/DBSCAN/Hierarchical]\n",
        "\n",
        "### Notlar\n",
        "\n",
        "- Silhouette score en önemli metrik olarak kullanıldı\n",
        "- Business rules'a göre 4 cluster tercih edildi\n",
        "- Final model seçimi için Model Evaluation notebook'una bakın\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
